import { Hono } from 'hono';
import { cors } from 'hono/cors';
import { logger } from 'hono/logger';
import {
  GeminiClient,
  LocalLLMClient,
  FallbackLLMClient,
  type LLMClient,
  type CacheStorage,
  type CacheMetadata,
  type CacheListItem,
  type UsageLogger,
  type UsageEntry,
  type UsageStats,
  type UsageOperation,
  type ContentStore,
  MnemoConfigSchema,
  calculateCost,
  UrlAdapter,
  // RAG support
  type VectorizeClient,
  type EmbeddingClient,
  type RepoIndexStorage,
  type RepoIndexMetadata,
  type Vector,
  type VectorMatch,
  type VectorQueryResult,
  type VectorizeQueryOptions,
  type ChunkStorage,
  type StoredChunk,
} from '@mnemo/core';
import { MnemoMCPServer, toolDefinitions } from '@mnemo/mcp-server';

// Local model configuration
const LOCAL_MODEL_URL = 'https://vllm.shiftaltcreate.com';
const LOCAL_MODEL_NAME = 'nemotron-3-nano';
const LOCAL_MODEL_MAX_TOKENS = 262144; // 256K context

// Env interface is defined in worker-configuration.d.ts (generated by wrangler types)

// Create app with bindings type
const app = new Hono<{ Bindings: Env }>();

// Middleware
app.use('*', cors());
app.use('*', logger());

// ============================================================================
// Rate Limiting with KV Storage
// ============================================================================

interface RateLimitData {
  requests: number;
  tokens: number;
  warningEmailSent: boolean;
  blockedEmailSent: boolean;
}

function getDateKey(): string {
  return new Date().toISOString().split('T')[0]; // YYYY-MM-DD
}

async function getRateLimitData(kv: KVNamespace): Promise<RateLimitData> {
  const key = `ratelimit:daily:${getDateKey()}`;
  const data = await kv.get(key, 'json');
  return (data as RateLimitData) || {
    requests: 0,
    tokens: 0,
    warningEmailSent: false,
    blockedEmailSent: false,
  };
}

async function updateRateLimitData(
  kv: KVNamespace,
  data: RateLimitData
): Promise<void> {
  const key = `ratelimit:daily:${getDateKey()}`;
  // TTL of 48 hours (172800 seconds)
  await kv.put(key, JSON.stringify(data), { expirationTtl: 172800 });
}

// Burst tracking (in-memory, resets on worker restart)
interface BurstEntry {
  count: number;
  windowStart: number;
}
const burstTracker = new Map<string, BurstEntry>();

async function checkBurstLimit(
  ip: string,
  env: Env
): Promise<{ exceeded: boolean; count: number }> {
  const now = Date.now();
  const windowMs = 60000; // 1 minute
  const maxBurst = 50;

  const entry = burstTracker.get(ip);

  if (!entry || now - entry.windowStart > windowMs) {
    burstTracker.set(ip, { count: 1, windowStart: now });
    return { exceeded: false, count: 1 };
  }

  entry.count++;

  if (entry.count > maxBurst) {
    // Send burst alert (async, don't wait)
    sendAlertEmail(
      env,
      'Burst Rate Limit Alert',
      `Unusual burst detected: ${entry.count} requests in 1 minute from IP ${ip}`
    ).catch(console.error);
    return { exceeded: true, count: entry.count };
  }

  return { exceeded: false, count: entry.count };
}

// ============================================================================
// Email Alerts via Resend
// ============================================================================

async function sendAlertEmail(
  env: Env,
  subject: string,
  message: string
): Promise<void> {
  const apiKey = env.RESEND_API_KEY;
  const alertEmail = env.ALERT_EMAIL || 'chris@solamp.io';

  if (!apiKey) {
    console.warn('RESEND_API_KEY not configured, skipping email alert');
    console.log(`Alert: ${subject} - ${message}`);
    return;
  }

  try {
    const response = await fetch('https://api.resend.com/emails', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        from: 'Mnemo Alerts <alerts@solamp.io>',
        to: [alertEmail],
        subject: `[Mnemo] ${subject}`,
        text: `${message}\n\nTimestamp: ${new Date().toISOString()}`,
      }),
    });

    if (!response.ok) {
      const error = await response.text();
      console.error('Failed to send alert email:', error);
    }
  } catch (error) {
    console.error('Error sending alert email:', error);
  }
}

// ============================================================================
// Rate Limiting Middleware
// ============================================================================

const rateLimitMiddleware = () => {
  return async (c: any, next: any) => {
    const env = c.env as Env;
    const kv = env.RATE_LIMIT_KV;
    const ip = c.req.header('CF-Connecting-IP') || 'unknown';

    // Check burst limit first
    const burst = await checkBurstLimit(ip, env);
    if (burst.exceeded) {
      return c.json({
        error: 'Rate limit exceeded',
        message: 'Too many requests in a short period. Please slow down.',
      }, 429);
    }

    // Get daily limits from env
    const dailyRequestLimit = parseInt(env.DAILY_REQUEST_LIMIT || '500', 10);
    const dailyTokenLimit = parseInt(env.DAILY_TOKEN_LIMIT || '2000000', 10);

    // Get current usage
    const data = await getRateLimitData(kv);

    // Check if blocked
    if (data.requests >= dailyRequestLimit) {
      // Send blocked email if not sent yet
      if (!data.blockedEmailSent) {
        data.blockedEmailSent = true;
        await updateRateLimitData(kv, data);
        sendAlertEmail(
          env,
          'Daily Request Limit EXCEEDED',
          `Daily request limit of ${dailyRequestLimit} has been reached.\nTotal requests today: ${data.requests}\nTotal tokens today: ${data.tokens}`
        ).catch(console.error);
      }

      return c.json({
        error: 'Daily limit exceeded',
        message: 'Daily request limit reached. Please try again tomorrow.',
        limit: dailyRequestLimit,
        used: data.requests,
      }, 429);
    }

    // Check warning threshold (80%)
    const warningThreshold = Math.floor(dailyRequestLimit * 0.8);
    if (data.requests >= warningThreshold && !data.warningEmailSent) {
      data.warningEmailSent = true;
      await updateRateLimitData(kv, data);
      sendAlertEmail(
        env,
        'Daily Request Limit Warning (80%)',
        `Approaching daily request limit.\nCurrent requests: ${data.requests} / ${dailyRequestLimit} (${Math.round(data.requests / dailyRequestLimit * 100)}%)\nTotal tokens today: ${data.tokens}`
      ).catch(console.error);
    }

    // Increment request count
    data.requests++;
    await updateRateLimitData(kv, data);

    // Store data in context for token tracking after response
    c.set('rateLimitData', data);

    return await next();
  };
};

// ============================================================================
// Write Protection
// ============================================================================

// Tools that require passphrase (write operations)
const WRITE_TOOLS = ['context_load', 'context_refresh', 'context_evict'];

// Tools that don't require passphrase (read operations)
const READ_TOOLS = ['context_list', 'context_query', 'context_stats'];

/**
 * Check if a tool call requires passphrase and validate it
 * Returns error message if invalid, null if valid
 */
function checkWritePassphrase(
  toolName: string,
  args: Record<string, unknown>,
  env: Env
): string | null {
  // If not a write tool, no passphrase needed
  if (!WRITE_TOOLS.includes(toolName)) {
    return null;
  }

  // If no passphrase configured, allow all writes (backwards compatible)
  const expectedPassphrase = env.WRITE_PASSPHRASE;
  if (!expectedPassphrase) {
    return null;
  }

  // Check passphrase
  const providedPassphrase = args.passphrase as string | undefined;
  if (!providedPassphrase) {
    return 'Write operation requires passphrase';
  }

  if (providedPassphrase !== expectedPassphrase) {
    return 'Invalid passphrase';
  }

  return null;
}

/**
 * Extract tool name and arguments from MCP request
 */
function extractToolInfo(request: any): { toolName: string; args: Record<string, unknown> } | null {
  if (request?.method !== 'tools/call') {
    return null;
  }

  const params = request?.params;
  if (!params?.name || typeof params.name !== 'string') {
    return null;
  }

  return {
    toolName: params.name,
    args: (params.arguments as Record<string, unknown>) || {},
  };
}

// ============================================================================
// Routes
// ============================================================================

// Health check (no rate limit)
app.get('/health', async (c) => {
  // Check if local model is available
  let localModelAvailable = false;
  try {
    const response = await fetch(`${LOCAL_MODEL_URL}/v1/models`, {
      signal: AbortSignal.timeout(5000),
    });
    localModelAvailable = response.ok;
  } catch {
    localModelAvailable = false;
  }

  return c.json({
    status: 'ok',
    service: 'mnemo',
    version: '0.2.0',
    environment: c.env.ENVIRONMENT,
    models: {
      primary: {
        name: LOCAL_MODEL_NAME,
        url: LOCAL_MODEL_URL,
        available: localModelAvailable,
      },
      fallback: {
        name: 'gemini-2.0-flash-001',
        available: !!c.env.GEMINI_API_KEY,
      },
    },
  });
});

// Service info (no rate limit)
app.get('/', (c) => {
  return c.json({
    name: 'mnemo',
    version: '0.1.0',
    description: 'Extended memory for AI assistants via Gemini context caching',
    endpoints: {
      health: 'GET /health',
      tools: 'GET /tools',
      mcp: 'POST /mcp',
    },
  });
});

// List available tools (no rate limit)
app.get('/tools', (c) => {
  return c.json({ tools: toolDefinitions });
});

// MCP protocol endpoint (rate limited)
app.post('/mcp', rateLimitMiddleware(), async (c) => {
  const server = createMCPServer(c.env);

  try {
    const request = await c.req.json();

    // Check write passphrase for tool calls
    const toolInfo = extractToolInfo(request);
    if (toolInfo) {
      const error = checkWritePassphrase(toolInfo.toolName, toolInfo.args, c.env);
      if (error) {
        return c.json({
          jsonrpc: '2.0',
          id: request.id ?? null,
          result: {
            content: [{ type: 'text', text: `Error: ${error}` }],
            isError: true,
          },
        });
      }
    }

    const response = await server.handleRequest(request);
    return c.json(response);
  } catch (error) {
    return c.json({
      jsonrpc: '2.0',
      id: null,
      error: {
        code: -32700,
        message: 'Parse error',
      },
    }, 400);
  }
});

// Direct tool invocation (rate limited)
app.post('/tools/:toolName', rateLimitMiddleware(), async (c) => {
  const toolName = c.req.param('toolName');
  const server = createMCPServer(c.env);

  try {
    const args = await c.req.json();

    // Check write passphrase
    const passphraseError = checkWritePassphrase(toolName, args, c.env);
    if (passphraseError) {
      return c.json({ error: passphraseError }, 403);
    }

    const response = await server.handleRequest({
      jsonrpc: '2.0',
      id: 1,
      method: 'tools/call',
      params: {
        name: toolName,
        arguments: args,
      },
    });

    // Extract result from MCP response
    if ('result' in response && response.result) {
      return c.json(response.result);
    }
    if ('error' in response && response.error) {
      return c.json({ error: response.error.message }, 400);
    }
    return c.json(response);
  } catch (error) {
    return c.json({ error: 'Invalid request' }, 400);
  }
});

// ============================================================================
// Async Query Endpoints
// ============================================================================

// Submit an async query - returns immediately with jobId
// The actual processing happens via waitUntil (for short tasks) or can be polled
app.post('/query/async', rateLimitMiddleware(), async (c) => {
  const jobStore = new D1AsyncJobStore(c.env.DB);

  try {
    const body = await c.req.json<{
      alias: string;
      query: string;
      maxTokens?: number;
      temperature?: number;
      wait?: boolean; // If true, wait for result instead of returning immediately
    }>();

    if (!body.alias || !body.query) {
      return c.json({ error: 'Missing required fields: alias and query' }, 400);
    }

    // Create job with 10 minute expiry
    const jobId = crypto.randomUUID();
    const expiresAt = new Date(Date.now() + 10 * 60 * 1000);

    await jobStore.create({
      id: jobId,
      cacheAlias: body.alias,
      query: body.query,
      status: 'pending',
      expiresAt,
    });

    const server = createMCPServer(c.env);

    // If wait=true, process synchronously and return result
    if (body.wait) {
      await processAsyncQuery(jobStore, server, jobId, body.alias, body.query, {
        maxTokens: body.maxTokens,
        temperature: body.temperature,
      });

      const job = await jobStore.get(jobId);
      if (job?.status === 'complete' && job.result) {
        return c.json({
          jobId,
          status: 'complete',
          result: JSON.parse(job.result),
        });
      } else if (job?.status === 'failed') {
        return c.json({
          jobId,
          status: 'failed',
          error: job.error,
        }, 500);
      }
    }

    // Otherwise, start processing in background using waitUntil
    // Note: waitUntil may not complete for very long tasks (2+ minutes)
    c.executionCtx.waitUntil(
      processAsyncQuery(jobStore, server, jobId, body.alias, body.query, {
        maxTokens: body.maxTokens,
        temperature: body.temperature,
      })
    );

    return c.json({
      jobId,
      status: 'pending',
      statusUrl: `/query/status/${jobId}`,
    }, 202);
  } catch (error) {
    console.error('Failed to create async job:', error);
    return c.json({ error: 'Failed to create async job' }, 500);
  }
});

// Poll for async query result
app.get('/query/status/:jobId', async (c) => {
  const jobId = c.req.param('jobId');
  const jobStore = new D1AsyncJobStore(c.env.DB);

  try {
    const job = await jobStore.get(jobId);

    if (!job) {
      return c.json({ error: 'Job not found' }, 404);
    }

    // Check if expired
    if (new Date() > job.expiresAt) {
      return c.json({ error: 'Job expired' }, 410);
    }

    const response: Record<string, unknown> = {
      jobId: job.id,
      status: job.status,
      createdAt: job.createdAt.toISOString(),
      updatedAt: job.updatedAt.toISOString(),
    };

    if (job.status === 'complete' && job.result) {
      response.result = JSON.parse(job.result);
    } else if (job.status === 'failed' && job.error) {
      response.error = job.error;
    }

    return c.json(response);
  } catch (error) {
    console.error('Failed to get job status:', error);
    return c.json({ error: 'Failed to get job status' }, 500);
  }
});

/**
 * Process async query in background
 */
async function processAsyncQuery(
  jobStore: D1AsyncJobStore,
  server: MnemoMCPServer,
  jobId: string,
  alias: string,
  query: string,
  options: { maxTokens?: number; temperature?: number }
): Promise<void> {
  try {
    // Mark as processing
    await jobStore.updateStatus(jobId, 'processing');

    // Execute the query via MCP server
    const response = await server.handleRequest({
      jsonrpc: '2.0',
      id: 1,
      method: 'tools/call',
      params: {
        name: 'context_query',
        arguments: {
          alias,
          query,
          maxTokens: options.maxTokens,
          temperature: options.temperature,
        },
      },
    });

    // Check for errors in response
    if ('error' in response && response.error) {
      await jobStore.updateStatus(jobId, 'failed', undefined, response.error.message);
      return;
    }

    // Extract result from MCP response
    if ('result' in response && response.result) {
      const mcpResult = response.result as { content: Array<{ type: string; text: string }>; isError?: boolean };

      if (mcpResult.isError) {
        const errorText = mcpResult.content[0]?.text ?? 'Unknown error';
        await jobStore.updateStatus(jobId, 'failed', undefined, errorText);
        return;
      }

      // Parse the JSON result from MCP response
      const resultText = mcpResult.content[0]?.text;
      if (resultText) {
        await jobStore.updateStatus(jobId, 'complete', resultText);
      } else {
        await jobStore.updateStatus(jobId, 'failed', undefined, 'Empty response from query');
      }
    } else {
      await jobStore.updateStatus(jobId, 'failed', undefined, 'Invalid response format');
    }
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : 'Unknown error';
    console.error('Async query failed:', errorMessage);
    await jobStore.updateStatus(jobId, 'failed', undefined, errorMessage);
  }
}

// Usage stats endpoint (for monitoring)
app.get('/stats', async (c) => {
  const kv = c.env.RATE_LIMIT_KV;
  const data = await getRateLimitData(kv);
  const dailyRequestLimit = parseInt(c.env.DAILY_REQUEST_LIMIT || '500', 10);
  const dailyTokenLimit = parseInt(c.env.DAILY_TOKEN_LIMIT || '2000000', 10);

  return c.json({
    date: getDateKey(),
    requests: {
      used: data.requests,
      limit: dailyRequestLimit,
      remaining: Math.max(0, dailyRequestLimit - data.requests),
      percentage: Math.round((data.requests / dailyRequestLimit) * 100),
    },
    tokens: {
      used: data.tokens,
      limit: dailyTokenLimit,
      remaining: Math.max(0, dailyTokenLimit - data.tokens),
      percentage: Math.round((data.tokens / dailyTokenLimit) * 100),
    },
  });
});

// ============================================================================
// Helpers
// ============================================================================

function createMCPServer(env: Env): MnemoMCPServer {
  const config = MnemoConfigSchema.parse({
    geminiApiKey: env.GEMINI_API_KEY,
  });

  // Create D1 content store for local model caching (persists across worker invocations)
  const contentStore = new D1ContentStore(env.DB);

  // Create local model client (Nemotron via vLLM tunnel)
  const localClient = new LocalLLMClient(
    {
      baseUrl: LOCAL_MODEL_URL,
      model: LOCAL_MODEL_NAME,
      maxContextTokens: LOCAL_MODEL_MAX_TOKENS,
      timeout: 120000,
    },
    contentStore
  );

  // Use fallback client only if Gemini API key is available
  let llmClient: LLMClient;
  if (env.GEMINI_API_KEY) {
    const geminiClient = new GeminiClient(config);
    llmClient = new FallbackLLMClient({
      primary: localClient,
      fallback: geminiClient,
      autoFallbackForLargeContext: true,
      onFallbackNeeded: async (event) => {
        console.log(`Fallback to Gemini: ${event.reason} - ${event.details ?? 'N/A'}`);
        return true; // Auto-approve fallback in CF worker
      },
    });
  } else {
    console.log('No GEMINI_API_KEY configured - using local model only (no fallback)');
    llmClient = localClient;
  }

  const storage = new D1CacheStorage(env.DB);
  const usageLogger = new D1UsageLogger(env.DB);
  // Workers have a 50 subrequest limit, use 40 to leave headroom
  const urlAdapter = new UrlAdapter({ maxSubrequests: 40 });

  // RAG support (v0.3) - create clients if bindings are available
  const vectorizeClient = env.VECTORIZE ? new CloudflareVectorizeClient(env.VECTORIZE) : undefined;
  const embeddingClient = env.AI ? new WorkersAIEmbeddingClient(env.AI) : undefined;
  const repoIndexStorage = new D1RepoIndexStorage(env.DB);
  const chunkStorage = new D1ChunkStorage(env.DB);

  return new MnemoMCPServer({
    geminiClient: llmClient,
    storage,
    usageLogger,
    urlAdapter,
    vectorizeClient,
    embeddingClient,
    repoIndexStorage,
    chunkStorage,
  });
}

// ============================================================================
// D1 Storage Implementation
// ============================================================================

class D1CacheStorage implements CacheStorage {
  constructor(private db: D1Database) {}

  async save(metadata: CacheMetadata): Promise<void> {
    await this.db
      .prepare(
        `INSERT INTO caches (id, alias, gemini_cache_name, source, token_count, model, expires_at)
         VALUES (?, ?, ?, ?, ?, ?, ?)
         ON CONFLICT(alias) DO UPDATE SET
           gemini_cache_name = excluded.gemini_cache_name,
           source = excluded.source,
           token_count = excluded.token_count,
           model = excluded.model,
           expires_at = excluded.expires_at`
      )
      .bind(
        crypto.randomUUID(),
        metadata.alias,
        metadata.name,
        metadata.source,
        metadata.tokenCount,
        metadata.model ?? null,
        metadata.expiresAt.toISOString()
      )
      .run();
  }

  async getByAlias(alias: string): Promise<CacheMetadata | null> {
    const result = await this.db
      .prepare('SELECT * FROM caches WHERE alias = ?')
      .bind(alias)
      .first<{
        id: string;
        alias: string;
        gemini_cache_name: string;
        source: string;
        token_count: number;
        model: string | null;
        created_at: string;
        expires_at: string;
      }>();

    if (!result) return null;

    return {
      name: result.gemini_cache_name,
      alias: result.alias,
      tokenCount: result.token_count,
      createdAt: new Date(result.created_at),
      expiresAt: new Date(result.expires_at),
      source: result.source,
      model: result.model ?? undefined,
    };
  }

  async getByName(name: string): Promise<CacheMetadata | null> {
    const result = await this.db
      .prepare('SELECT * FROM caches WHERE gemini_cache_name = ?')
      .bind(name)
      .first<{
        id: string;
        alias: string;
        gemini_cache_name: string;
        source: string;
        token_count: number;
        model: string | null;
        created_at: string;
        expires_at: string;
      }>();

    if (!result) return null;

    return {
      name: result.gemini_cache_name,
      alias: result.alias,
      tokenCount: result.token_count,
      createdAt: new Date(result.created_at),
      expiresAt: new Date(result.expires_at),
      source: result.source,
      model: result.model ?? undefined,
    };
  }

  async list(): Promise<CacheListItem[]> {
    const results = await this.db
      .prepare('SELECT alias, token_count, expires_at, source FROM caches ORDER BY created_at DESC')
      .all<{
        alias: string;
        token_count: number;
        expires_at: string;
        source: string;
      }>();

    return (results.results ?? []).map((row) => ({
      alias: row.alias,
      tokenCount: row.token_count,
      expiresAt: new Date(row.expires_at),
      source: row.source,
    }));
  }

  async deleteByAlias(alias: string): Promise<boolean> {
    const result = await this.db
      .prepare('DELETE FROM caches WHERE alias = ?')
      .bind(alias)
      .run();
    return (result.meta?.changes ?? 0) > 0;
  }

  async update(alias: string, updates: Partial<CacheMetadata>): Promise<void> {
    const sets: string[] = [];
    const values: unknown[] = [];

    if (updates.expiresAt) {
      sets.push('expires_at = ?');
      values.push(updates.expiresAt.toISOString());
    }
    if (updates.tokenCount !== undefined) {
      sets.push('token_count = ?');
      values.push(updates.tokenCount);
    }

    if (sets.length === 0) return;

    values.push(alias);
    await this.db
      .prepare(`UPDATE caches SET ${sets.join(', ')} WHERE alias = ?`)
      .bind(...values)
      .run();
  }
}

// ============================================================================
// D1 Content Store Implementation (for local LLM caching)
// ============================================================================

class D1ContentStore implements ContentStore {
  constructor(private db: D1Database) {}

  async set(key: string, content: string, ttl = 3600): Promise<void> {
    const expiresAt = new Date(Date.now() + ttl * 1000).toISOString();
    await this.db
      .prepare(
        `INSERT INTO cache_content (cache_name, content, expires_at)
         VALUES (?, ?, ?)
         ON CONFLICT(cache_name) DO UPDATE SET
           content = excluded.content,
           expires_at = excluded.expires_at`
      )
      .bind(key, content, expiresAt)
      .run();
  }

  async get(key: string): Promise<string | null> {
    const result = await this.db
      .prepare(
        `SELECT content, expires_at FROM cache_content WHERE cache_name = ?`
      )
      .bind(key)
      .first<{ content: string; expires_at: string }>();

    if (!result) return null;

    // Check expiration
    if (new Date(result.expires_at) < new Date()) {
      // Expired, delete and return null
      await this.delete(key);
      return null;
    }

    return result.content;
  }

  async delete(key: string): Promise<boolean> {
    const result = await this.db
      .prepare('DELETE FROM cache_content WHERE cache_name = ?')
      .bind(key)
      .run();
    return (result.meta?.changes ?? 0) > 0;
  }
}

// ============================================================================
// D1 Usage Logger Implementation
// ============================================================================

// ============================================================================
// Async Job Types and Storage
// ============================================================================

type AsyncJobStatus = 'pending' | 'processing' | 'complete' | 'failed';

interface AsyncJob {
  id: string;
  cacheAlias: string;
  query: string;
  status: AsyncJobStatus;
  result?: string; // JSON-encoded result
  error?: string;
  createdAt: Date;
  updatedAt: Date;
  expiresAt: Date;
}

interface AsyncJobStore {
  create(job: Omit<AsyncJob, 'createdAt' | 'updatedAt'>): Promise<string>;
  get(id: string): Promise<AsyncJob | null>;
  updateStatus(id: string, status: AsyncJobStatus, result?: string, error?: string): Promise<void>;
  cleanupExpired(): Promise<number>;
}

class D1AsyncJobStore implements AsyncJobStore {
  constructor(private db: D1Database) {}

  async create(job: Omit<AsyncJob, 'createdAt' | 'updatedAt'>): Promise<string> {
    await this.db
      .prepare(
        `INSERT INTO async_jobs (id, cache_alias, query, status, expires_at)
         VALUES (?, ?, ?, ?, ?)`
      )
      .bind(job.id, job.cacheAlias, job.query, job.status, job.expiresAt.toISOString())
      .run();
    return job.id;
  }

  async get(id: string): Promise<AsyncJob | null> {
    const result = await this.db
      .prepare('SELECT * FROM async_jobs WHERE id = ?')
      .bind(id)
      .first<{
        id: string;
        cache_alias: string;
        query: string;
        status: string;
        result: string | null;
        error: string | null;
        created_at: string;
        updated_at: string;
        expires_at: string;
      }>();

    if (!result) return null;

    return {
      id: result.id,
      cacheAlias: result.cache_alias,
      query: result.query,
      status: result.status as AsyncJobStatus,
      result: result.result ?? undefined,
      error: result.error ?? undefined,
      createdAt: new Date(result.created_at),
      updatedAt: new Date(result.updated_at),
      expiresAt: new Date(result.expires_at),
    };
  }

  async updateStatus(id: string, status: AsyncJobStatus, result?: string, error?: string): Promise<void> {
    await this.db
      .prepare(
        `UPDATE async_jobs
         SET status = ?, result = ?, error = ?, updated_at = CURRENT_TIMESTAMP
         WHERE id = ?`
      )
      .bind(status, result ?? null, error ?? null, id)
      .run();
  }

  async cleanupExpired(): Promise<number> {
    const result = await this.db
      .prepare('DELETE FROM async_jobs WHERE expires_at < CURRENT_TIMESTAMP')
      .run();
    return result.meta?.changes ?? 0;
  }
}

// ============================================================================
// D1 Usage Logger Implementation
// ============================================================================

class D1UsageLogger implements UsageLogger {
  constructor(private db: D1Database) {}

  async log(entry: Omit<UsageEntry, 'createdAt'>): Promise<void> {
    await this.db
      .prepare(
        `INSERT INTO usage_logs (cache_id, operation, tokens_used, cached_tokens_used)
         VALUES (?, ?, ?, ?)`
      )
      .bind(entry.cacheId, entry.operation, entry.tokensUsed, entry.cachedTokensUsed)
      .run();
  }

  async getStats(cacheId?: string): Promise<UsageStats> {
    const whereClause = cacheId ? 'WHERE cache_id = ?' : '';

    // Get totals
    const totalsStmt = this.db.prepare(
      `SELECT
        COUNT(*) as total_ops,
        COALESCE(SUM(tokens_used), 0) as total_tokens,
        COALESCE(SUM(cached_tokens_used), 0) as total_cached
       FROM usage_logs ${whereClause}`
    );
    const totals = cacheId
      ? await totalsStmt.bind(cacheId).first<{ total_ops: number; total_tokens: number; total_cached: number }>()
      : await totalsStmt.first<{ total_ops: number; total_tokens: number; total_cached: number }>();

    // Get breakdown by operation
    const byOpStmt = this.db.prepare(
      `SELECT
        operation,
        COUNT(*) as count,
        COALESCE(SUM(tokens_used), 0) as tokens_used,
        COALESCE(SUM(cached_tokens_used), 0) as cached_tokens_used
       FROM usage_logs ${whereClause}
       GROUP BY operation`
    );
    const byOpResults = cacheId
      ? await byOpStmt.bind(cacheId).all<{
          operation: string;
          count: number;
          tokens_used: number;
          cached_tokens_used: number;
        }>()
      : await byOpStmt.all<{
          operation: string;
          count: number;
          tokens_used: number;
          cached_tokens_used: number;
        }>();

    const byOperation: Record<UsageOperation, { count: number; tokensUsed: number; cachedTokensUsed: number }> = {
      load: { count: 0, tokensUsed: 0, cachedTokensUsed: 0 },
      query: { count: 0, tokensUsed: 0, cachedTokensUsed: 0 },
      evict: { count: 0, tokensUsed: 0, cachedTokensUsed: 0 },
      refresh: { count: 0, tokensUsed: 0, cachedTokensUsed: 0 },
    };

    for (const row of byOpResults.results ?? []) {
      const op = row.operation as UsageOperation;
      if (op in byOperation) {
        byOperation[op] = {
          count: row.count,
          tokensUsed: row.tokens_used,
          cachedTokensUsed: row.cached_tokens_used,
        };
      }
    }

    const totalTokens = totals?.total_tokens ?? 0;
    const totalCached = totals?.total_cached ?? 0;

    return {
      totalOperations: totals?.total_ops ?? 0,
      totalTokensUsed: totalTokens,
      totalCachedTokensUsed: totalCached,
      estimatedCost: calculateCost(totalTokens, totalCached),
      byOperation,
    };
  }

  async getRecent(limit = 100): Promise<UsageEntry[]> {
    const results = await this.db
      .prepare(
        `SELECT cache_id, operation, tokens_used, cached_tokens_used, created_at
         FROM usage_logs
         ORDER BY created_at DESC
         LIMIT ?`
      )
      .bind(limit)
      .all<{
        cache_id: string;
        operation: string;
        tokens_used: number;
        cached_tokens_used: number;
        created_at: string;
      }>();

    return (results.results ?? []).map((row) => ({
      cacheId: row.cache_id,
      operation: row.operation as UsageOperation,
      tokensUsed: row.tokens_used,
      cachedTokensUsed: row.cached_tokens_used,
      createdAt: new Date(row.created_at),
    }));
  }
}

// ============================================================================
// Cloudflare Workers AI Embedding Client
// ============================================================================

class WorkersAIEmbeddingClient implements EmbeddingClient {
  private ai: Ai;
  private model = '@cf/baai/bge-base-en-v1.5';
  private batchSize = 100; // Workers AI limit

  constructor(ai: Ai) {
    this.ai = ai;
  }

  async embed(texts: string[]): Promise<number[][]> {
    if (texts.length === 0) {
      return [];
    }

    // Process in batches
    const allEmbeddings: number[][] = [];

    for (let i = 0; i < texts.length; i += this.batchSize) {
      const batch = texts.slice(i, i + this.batchSize);

      const result = await this.ai.run(this.model as keyof AiModels, {
        text: batch,
      }) as { data: number[][] };

      allEmbeddings.push(...result.data);
    }

    return allEmbeddings;
  }
}

// ============================================================================
// Cloudflare Vectorize Client
// ============================================================================

class CloudflareVectorizeClient implements VectorizeClient {
  private vectorize: VectorizeIndex;
  private batchSize = 1000; // Vectorize upsert limit

  constructor(vectorize: VectorizeIndex) {
    this.vectorize = vectorize;
  }

  async insert(vectors: Vector[]): Promise<{ count: number }> {
    if (vectors.length === 0) {
      return { count: 0 };
    }

    let totalInserted = 0;

    // Process in batches
    for (let i = 0; i < vectors.length; i += this.batchSize) {
      const batch = vectors.slice(i, i + this.batchSize);

      // Convert to Vectorize format
      const vectorizeVectors: VectorizeVector[] = batch.map((v) => ({
        id: v.id,
        values: v.values,
        metadata: v.metadata as Record<string, VectorizeVectorMetadata>,
      }));

      const result = await this.vectorize.upsert(vectorizeVectors);
      totalInserted += result.count;
    }

    return { count: totalInserted };
  }

  async query(vector: number[], options: VectorizeQueryOptions = {}): Promise<VectorQueryResult> {
    const {
      topK = 5,
      filter,
      returnMetadata = true,
    } = options;

    const result = await this.vectorize.query(vector, {
      topK,
      filter: filter as VectorizeVectorMetadataFilter,
      returnMetadata: returnMetadata ? 'all' : 'none',
    });

    return {
      matches: result.matches.map((m) => ({
        id: m.id,
        score: m.score,
        metadata: m.metadata as Record<string, unknown>,
      })),
      count: result.count,
    };
  }

  async deleteByIds(ids: string[]): Promise<{ count: number }> {
    if (ids.length === 0) {
      return { count: 0 };
    }

    const result = await this.vectorize.deleteByIds(ids);
    return { count: result.count };
  }

  async deleteByFilter(_filter: Record<string, string | number>): Promise<{ count: number }> {
    // Vectorize doesn't support delete by filter directly
    throw new Error('deleteByFilter not directly supported. Query first, then delete by IDs.');
  }
}

// ============================================================================
// D1 Repo Index Storage
// ============================================================================

class D1RepoIndexStorage implements RepoIndexStorage {
  constructor(private db: D1Database) {}

  async save(metadata: RepoIndexMetadata): Promise<void> {
    await this.db
      .prepare(
        `INSERT INTO repo_indexes (id, alias, source, chunk_count, total_tokens, file_count, expires_at, status)
         VALUES (?, ?, ?, ?, ?, ?, ?, ?)
         ON CONFLICT(alias) DO UPDATE SET
           source = excluded.source,
           chunk_count = excluded.chunk_count,
           total_tokens = excluded.total_tokens,
           file_count = excluded.file_count,
           indexed_at = CURRENT_TIMESTAMP,
           expires_at = excluded.expires_at,
           status = excluded.status`
      )
      .bind(
        metadata.id,
        metadata.alias,
        metadata.source,
        metadata.chunkCount,
        metadata.totalTokens,
        metadata.fileCount,
        metadata.expiresAt?.toISOString() ?? null,
        metadata.status
      )
      .run();
  }

  async getByAlias(alias: string): Promise<RepoIndexMetadata | null> {
    const result = await this.db
      .prepare('SELECT * FROM repo_indexes WHERE alias = ?')
      .bind(alias)
      .first<{
        id: string;
        alias: string;
        source: string;
        chunk_count: number;
        total_tokens: number;
        file_count: number;
        indexed_at: string;
        expires_at: string | null;
        status: string;
      }>();

    if (!result) return null;

    return {
      id: result.id,
      alias: result.alias,
      source: result.source,
      chunkCount: result.chunk_count,
      totalTokens: result.total_tokens,
      fileCount: result.file_count,
      indexedAt: new Date(result.indexed_at),
      expiresAt: result.expires_at ? new Date(result.expires_at) : undefined,
      status: result.status as 'active' | 'indexing' | 'failed',
    };
  }

  async list(): Promise<RepoIndexMetadata[]> {
    const results = await this.db
      .prepare('SELECT * FROM repo_indexes ORDER BY indexed_at DESC')
      .all<{
        id: string;
        alias: string;
        source: string;
        chunk_count: number;
        total_tokens: number;
        file_count: number;
        indexed_at: string;
        expires_at: string | null;
        status: string;
      }>();

    return (results.results ?? []).map((row) => ({
      id: row.id,
      alias: row.alias,
      source: row.source,
      chunkCount: row.chunk_count,
      totalTokens: row.total_tokens,
      fileCount: row.file_count,
      indexedAt: new Date(row.indexed_at),
      expiresAt: row.expires_at ? new Date(row.expires_at) : undefined,
      status: row.status as 'active' | 'indexing' | 'failed',
    }));
  }

  async deleteByAlias(alias: string): Promise<boolean> {
    const result = await this.db
      .prepare('DELETE FROM repo_indexes WHERE alias = ?')
      .bind(alias)
      .run();
    return (result.meta?.changes ?? 0) > 0;
  }

  async updateStatus(alias: string, status: 'active' | 'indexing' | 'failed'): Promise<void> {
    await this.db
      .prepare('UPDATE repo_indexes SET status = ? WHERE alias = ?')
      .bind(status, alias)
      .run();
  }
}

/**
 * D1 implementation of ChunkStorage for storing chunk content
 */
class D1ChunkStorage implements ChunkStorage {
  constructor(private db: D1Database) {}

  async saveChunks(chunks: StoredChunk[]): Promise<void> {
    if (chunks.length === 0) return;

    // Batch insert chunks (D1 has 1MB limit per statement, so we batch)
    const batchSize = 100;
    for (let i = 0; i < chunks.length; i += batchSize) {
      const batch = chunks.slice(i, i + batchSize);
      const statements = batch.map((chunk) =>
        this.db
          .prepare(
            `INSERT INTO repo_chunks (id, repo_alias, file_path, chunk_index, content, start_line, end_line, token_count)
             VALUES (?, ?, ?, ?, ?, ?, ?, ?)
             ON CONFLICT(id) DO UPDATE SET
               content = excluded.content,
               start_line = excluded.start_line,
               end_line = excluded.end_line,
               token_count = excluded.token_count`
          )
          .bind(
            chunk.id,
            chunk.repoAlias,
            chunk.filePath,
            chunk.chunkIndex,
            chunk.content,
            chunk.startLine ?? null,
            chunk.endLine ?? null,
            chunk.tokenCount
          )
      );
      await this.db.batch(statements);
    }
  }

  async getChunksByIds(ids: string[]): Promise<StoredChunk[]> {
    if (ids.length === 0) return [];

    // D1 doesn't support array parameters, so we use IN with placeholders
    const placeholders = ids.map(() => '?').join(',');
    const result = await this.db
      .prepare(`SELECT * FROM repo_chunks WHERE id IN (${placeholders})`)
      .bind(...ids)
      .all<{
        id: string;
        repo_alias: string;
        file_path: string;
        chunk_index: number;
        content: string;
        start_line: number | null;
        end_line: number | null;
        token_count: number;
      }>();

    return (result.results ?? []).map((row) => ({
      id: row.id,
      repoAlias: row.repo_alias,
      filePath: row.file_path,
      chunkIndex: row.chunk_index,
      content: row.content,
      startLine: row.start_line ?? undefined,
      endLine: row.end_line ?? undefined,
      tokenCount: row.token_count,
    }));
  }

  async deleteByRepoAlias(repoAlias: string): Promise<number> {
    const result = await this.db
      .prepare('DELETE FROM repo_chunks WHERE repo_alias = ?')
      .bind(repoAlias)
      .run();
    return result.meta?.changes ?? 0;
  }
}

export default app;
